{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7cc9b431-b794-42fe-ad00-dd5d00c2853f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.11.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.31.0-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.4 MB 25.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from tensorflow) (61.2.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.22.4)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.6.0)\n",
      "Collecting flatbuffers>=2.0\n",
      "  Downloading flatbuffers-23.3.3-py2.py3-none-any.whl (26 kB)\n",
      "Collecting libclang>=13.0.0\n",
      "  Using cached libclang-15.0.6.1-py2.py3-none-manylinux2010_x86_64.whl (21.5 MB)\n",
      "Collecting absl-py>=1.0.0\n",
      "  Using cached absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Using cached tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Using cached gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Using cached keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.8/site-packages (from tensorflow) (21.3)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (4.4.0)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.15.0-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (81 kB)\n",
      "\u001b[K     |████████████████████████████████| 81 kB 213 kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: protobuf<3.20,>=3.9.2 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (3.14.0)\n",
      "Collecting termcolor>=1.1.0\n",
      "  Using cached termcolor-2.2.0-py3-none-any.whl (6.6 kB)\n",
      "Requirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.8/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting opt-einsum>=2.3.2\n",
      "  Using cached opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3\n",
      "  Using cached grpcio-1.51.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.8 MB)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Using cached tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Collecting tensorboard-plugin-wit>=1.6.0\n",
      "  Using cached tensorboard_plugin_wit-1.8.1-py3-none-any.whl (781 kB)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Collecting markdown>=2.6.8\n",
      "  Using cached Markdown-3.4.1-py3-none-any.whl (93 kB)\n",
      "Collecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.16.2-py2.py3-none-any.whl (177 kB)\n",
      "\u001b[K     |████████████████████████████████| 177 kB 65.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.8/site-packages (from tensorboard<2.12,>=2.11->tensorflow) (2.27.1)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Using cached Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
      "Collecting cachetools<6.0,>=2.0.0\n",
      "  Using cached cachetools-5.3.0-py3-none-any.whl (9.3 kB)\n",
      "Collecting rsa<5,>=3.1.4\n",
      "  Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Using cached pyasn1_modules-0.2.8-py2.py3-none-any.whl (155 kB)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Using cached requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /opt/conda/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (6.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.12,>=2.11->tensorflow) (3.7.0)\n",
      "Collecting pyasn1<0.5.0,>=0.4.6\n",
      "  Using cached pyasn1-0.4.8-py2.py3-none-any.whl (77 kB)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (2.0.12)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow) (3.3)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow) (3.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.8/site-packages (from packaging->tensorflow) (3.0.7)\n",
      "Installing collected packages: pyasn1, rsa, pyasn1-modules, cachetools, requests-oauthlib, google-auth, werkzeug, tensorboard-plugin-wit, tensorboard-data-server, markdown, grpcio, google-auth-oauthlib, absl-py, wrapt, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard, opt-einsum, libclang, keras, google-pasta, gast, flatbuffers, astunparse, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.0 flatbuffers-23.3.3 gast-0.4.0 google-auth-2.16.2 google-auth-oauthlib-0.4.6 google-pasta-0.2.0 grpcio-1.51.3 keras-2.11.0 libclang-15.0.6.1 markdown-3.4.1 opt-einsum-3.3.0 pyasn1-0.4.8 pyasn1-modules-0.2.8 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorboard-plugin-wit-1.8.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.2.0 werkzeug-2.2.3 wrapt-1.15.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bed1a470-cd59-465f-94f7-d598b12616d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pennylane\n",
      "  Downloading PennyLane-0.29.1-py3-none-any.whl (1.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 1.3 MB 34.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting semantic-version>=2.7\n",
      "  Using cached semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Collecting toml\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: appdirs in /opt/conda/lib/python3.8/site-packages (from pennylane) (1.4.4)\n",
      "Collecting autograd\n",
      "  Using cached autograd-1.5-py3-none-any.whl (48 kB)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.8/site-packages (from pennylane) (2.27.1)\n",
      "Collecting autoray>=0.3.1\n",
      "  Using cached autoray-0.6.1-py3-none-any.whl (47 kB)\n",
      "Requirement already satisfied: cachetools in /opt/conda/lib/python3.8/site-packages (from pennylane) (5.3.0)\n",
      "Collecting pennylane-lightning>=0.28\n",
      "  Using cached PennyLane_Lightning-0.29.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.5 MB)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from pennylane) (1.8.0)\n",
      "Requirement already satisfied: numpy<1.24 in /opt/conda/lib/python3.8/site-packages (from pennylane) (1.22.4)\n",
      "Requirement already satisfied: retworkx in /opt/conda/lib/python3.8/site-packages (from pennylane) (0.11.0)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.8/site-packages (from pennylane) (2.7.1)\n",
      "Requirement already satisfied: future>=0.15.2 in /opt/conda/lib/python3.8/site-packages (from autograd->pennylane) (0.18.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.8/site-packages (from requests->pennylane) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.8/site-packages (from requests->pennylane) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.8/site-packages (from requests->pennylane) (1.26.9)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /opt/conda/lib/python3.8/site-packages (from requests->pennylane) (2.0.12)\n",
      "Installing collected packages: toml, semantic-version, pennylane-lightning, autoray, autograd, pennylane\n",
      "Successfully installed autograd-1.5 autoray-0.6.1 pennylane-0.29.1 pennylane-lightning-0.29.0 semantic-version-2.10.0 toml-0.10.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pennylane"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2fcaaa8b-d2e7-46b2-9016-05804a26f393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.8/site-packages (1.2.0)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.2.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 9.8 MB 26.0 MB/s eta 0:00:01     |████████████                    | 3.7 MB 26.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.8/site-packages (from scikit-learn) (1.8.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.8/site-packages (from scikit-learn) (1.22.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn) (3.1.0)\n",
      "Installing collected packages: scikit-learn\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.2.0\n",
      "    Uninstalling scikit-learn-1.2.0:\n",
      "      Successfully uninstalled scikit-learn-1.2.0\n",
      "Successfully installed scikit-learn-1.2.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a5de4a31-1890-410e-9a1a-7769f0388540",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imbalanced-learn\n",
      "  Using cached imbalanced_learn-0.10.1-py3-none-any.whl (226 kB)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /opt/conda/lib/python3.8/site-packages (from imbalanced-learn) (1.22.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from imbalanced-learn) (3.1.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0.2 in /opt/conda/lib/python3.8/site-packages (from imbalanced-learn) (1.2.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.8/site-packages (from imbalanced-learn) (1.8.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.8/site-packages (from imbalanced-learn) (1.2.0)\n",
      "Installing collected packages: imbalanced-learn\n",
      "Successfully installed imbalanced-learn-0.10.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade imbalanced-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3a875e-e860-4f6f-bc61-4b8f8a01c930",
   "metadata": {},
   "source": [
    "**Note** : After you run these cells, please restart the kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb653f29-ec8e-4759-8476-734942a3cb08",
   "metadata": {},
   "source": [
    "## QML Challenge 01 : QSVM implementation\n",
    "____\n",
    "### Introduction\n",
    "Machine Learning has been a column of modern technology, present in almost all aspects of our lives and devices, to keep us safe, to improve our work, or serve as entertainment. A main task in Machine Learning is Quantum Image Processing. \n",
    "\n",
    "Your task in this challenge is to implement a QSVM model that classifies the Fashion MNIST dataset. However, QML has been rather limited in the context of image processing, due to the scale of the images. Your goal is to implement the QSVM as efficiently as possible. The criteria for assessment are :\n",
    "- **Circuit Volume (depth x width) :** This determines the size of your circuit, and as long as you have a good accuracy, the smaller the better!\n",
    "- **Accuracy :** The accuracy on the test dataset determines how well your model generalizes. So, try to get this accuracy as close to 1.0 as possible!\n",
    "- **Creativity :** Ahh! This is where one solution outshines another. How innovative your approach may be, will determine the true champion of this challenge, so let your quantum mind loose!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24c2bc73-9cd6-49cf-98db-208e08cfa961",
   "metadata": {},
   "source": [
    "A QSVM is made of three components :\n",
    "\n",
    "1) Encoding classical data to quantum data\n",
    "2) Constructing the quantum kernel\n",
    "3) Setting up a classical SVC\n",
    "\n",
    "You are free to use any package to solve the problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1778760c-4439-4c9f-926c-2a5aea16b587",
   "metadata": {},
   "source": [
    "We will provide the base imports for you, but feel free to add any additional imports!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2461a84d-9b34-4e2a-9f2a-6e6509d150f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<frozen importlib._bootstrap>:219: RuntimeWarning: scipy._lib.messagestream.MessageStream size changed, may indicate binary incompatibility. Expected 56 from C header, got 64 from PyObject\n",
      "2023-03-09 14:42:47.308385: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-09 14:42:47.472329: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-09 14:42:47.472364: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-09 14:42:48.457260: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-09 14:42:48.457410: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-09 14:42:48.457425: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All packages imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# General Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import math\n",
    "\n",
    "# Visualisation and graphs Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Scikit-Learn Imports\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# TensorFlow Import\n",
    "from tensorflow.keras.datasets import fashion_mnist\n",
    "\n",
    "# Quantum Imports\n",
    "# Qiskit Imports\n",
    "from qiskit import Aer, execute\n",
    "from qiskit.circuit import QuantumCircuit, Parameter, ParameterVector\n",
    "from qiskit.circuit.library import PauliFeatureMap, ZFeatureMap, ZZFeatureMap\n",
    "from qiskit.circuit.library import TwoLocal, NLocal, RealAmplitudes, EfficientSU2\n",
    "from qiskit.circuit.library import HGate, RXGate, RYGate, RZGate, CXGate, CRXGate, CRZGate\n",
    "from qiskit_machine_learning.kernels import QuantumKernel\n",
    "from qiskit_machine_learning.circuit.library import RawFeatureVector\n",
    "from qiskit.utils import algorithm_globals\n",
    "from qiskit_aer import StatevectorSimulator\n",
    "\n",
    "# PennyLane Imports\n",
    "import pennylane as qml\n",
    "from pennylane.templates import AngleEmbedding, AmplitudeEmbedding\n",
    "\n",
    "print(\"All packages imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176c25b6-d8c2-4a6e-9584-6611dba7d616",
   "metadata": {},
   "source": [
    "## Loading the dataset and preprocessing\n",
    "\n",
    "We will start by loading in the dataset, and splitting it into train and test samples. We have provided the training and test splits below. You may preprocess the dataset if it helps the solution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "211cdd56-1d1a-4b64-ab4a-46ad4156e6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, Y_train), (X_test, Y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14efeba5-a0b9-4a31-bfd2-e8517ee4fbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE YOUR CODE HERE (PREPROCESSING)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba7fc03f-73c0-43bd-a6b1-eb8fcc4f89a1",
   "metadata": {},
   "source": [
    "We can use the cell below to see the shape of the images. The Fashion MNIST dataset is made of 28 x 28 images, meaning an overall 784 pixels. How you encode your classical data into a quantum state will determine the size of your quantum circuit. Be creative in how you perform this step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3c1a9c81-117b-4edd-b68e-71afc7942264",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 60000 images, having a dimension of 28 x 28. The length of the image when flattened is 784.\n"
     ]
    }
   ],
   "source": [
    "print(f\"There are {X_train.shape[0]} images, having a dimension of {X_train.shape[1]} x {X_train.shape[2]}. The length of the image when flattened is {X_train.shape[1]*X_train.shape[2]}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f3af9546-59b4-4215-833d-ee8ae4751b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of qubits required for the 28 x 28 image is 10. We can encode 1024 parameters.\n"
     ]
    }
   ],
   "source": [
    "def qubits_required(image_size):\n",
    "    return int(np.ceil(np.log2(image_size.size)))\n",
    "\n",
    "def parameters(qubit_num):\n",
    "    return 2**int(qubit_num)\n",
    "\n",
    "print(f\"The number of qubits required for the {X_train.shape[1]} x {X_train.shape[2]} image is {qubits_required(X_train[0])}. We can encode {parameters(qubits_required(X_train[0]))} parameters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9de02c5-d0ec-4bec-87d9-e9599abaea5c",
   "metadata": {},
   "source": [
    "Ooh! You are 240 parameters short! How can we fix this problem without changing the image's semantics? Maybe try padding?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa73d30-5dfe-448b-9968-26d3d1eeab1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE YOUR CODE HERE (PADDING)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936466f9-e239-4ded-8be2-1d7b09f4e02f",
   "metadata": {},
   "source": [
    "## Encoding data\n",
    "\n",
    "Data Encoding the process of encoding classical data to quantum states using a quantum feature map.\n",
    "\n",
    "### **Quantum Feature Maps**\n",
    "\n",
    "A quantum feature map $\\phi(\\mathbf{x})$ is a map from the classical feature vector $\\mathbf{x}$ to the quantum state $|\\Phi(\\mathbf{x})\\rangle\\langle\\Phi(\\mathbf{x})|$. This is done by applying the unitary operation $\\mathcal{U}_{\\Phi(\\mathbf{x})}$ on the initial state $|0\\rangle^{n}$ where _n_ is the number of qubits being used for encoding.\n",
    "\n",
    "**ZZFeatureMap**\n",
    "\n",
    "ZZFeatureMap is an angle encoding PQC (Parameterized Quantum Circuit). Angle encoding is a 1-to-1 encoding, This means we can encode the two features using two qubits. \n",
    "\n",
    "`ZZFeatureMap` is conjectured to be hard to simulate classically and thus provides an incentive for using QML. This PQC can be implemented using short-depth circuits which makes it a great encoder for low dimensional datasets. \n",
    "  \n",
    " **RawFeatureVector**\n",
    " \n",
    "RFV is an amplitude encoding PQC. Unlike angle encoding, amplitude encoding only requires Log2(N) qubits, where _N_ is the number of features in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597f4341-a7b2-4709-a139-1f05dcae1969",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE YOUR CODE HERE (ENCODING)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f8e6b3c-d797-43f2-82e0-6456feaba52a",
   "metadata": {},
   "source": [
    "## Quantum Kernal Estimation\n",
    "\n",
    "A quantum feature map, $\\phi(\\mathbf{x})$, naturally gives rise to a quantum kernel, $k(\\mathbf{x}_i,\\mathbf{x}_j)= \\phi(\\mathbf{x}_j)^\\dagger\\phi(\\mathbf{x}_i)$, which can be seen as a measure of similarity: $k(\\mathbf{x}_i,\\mathbf{x}_j)$ is large when $\\mathbf{x}_i$ and $\\mathbf{x}_j$ are close. \n",
    "\n",
    "When considering finite data, we can represent the quantum kernel as a matrix: \n",
    "$K_{ij} = \\left| \\langle \\phi^\\dagger(\\mathbf{x}_j)| \\phi(\\mathbf{x}_i) \\rangle \\right|^{2}$. We can calculate each element of this kernel matrix on a quantum computer by calculating the transition amplitude:\n",
    "$$\n",
    "\\left| \\langle \\phi^\\dagger(\\mathbf{x}_j)| \\phi(\\mathbf{x}_i) \\rangle \\right|^{2} = \n",
    "\\left| \\langle 0^{\\otimes n} | \\mathbf{U_\\phi^\\dagger}(\\mathbf{x}_j) \\mathbf{U_\\phi}(\\mathbf{x_i}) | 0^{\\otimes n} \\rangle \\right|^{2}\n",
    "$$\n",
    "assuming the feature map is a parameterized quantum circuit, which can be described as a unitary transformation $\\mathbf{U_\\phi}(\\mathbf{x})$ on $n$ qubits. \n",
    "\n",
    "This provides us with an estimate of the quantum kernel matrix, which we can then use in a kernel machine learning algorithm, such as support vector classification.\n",
    "\n",
    "As discussed in [*Havlicek et al*.  Nature 567, 209-212 (2019)](https://www.nature.com/articles/s41586-019-0980-2), quantum kernel machine algorithms only have the potential of quantum advantage over classical approaches if the corresponding quantum kernel is hard to estimate **classically**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716ec9fb-ae81-4151-a267-d1a25cd4021d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE YOUR CODE HERE (KERNEL ESTIMATION)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dffda81-e08c-4d14-bbef-324dd33ca5d0",
   "metadata": {},
   "source": [
    "## Quantum Support Vector Classification\n",
    "\n",
    "\n",
    "Introduced in [*Havlicek et al*.  Nature 567, 209-212 (2019)](https://www.nature.com/articles/s41586-019-0980-2), the quantum kernel support vector classification algorithm consists of two steps:\n",
    "\n",
    "1. Build the train and test quantum kernel matrices.\n",
    "    1. For each pair of datapoints in the training dataset $\\mathbf{x}_{i},\\mathbf{x}_j$, apply the feature map and measure the transition probability: $ K_{ij} = \\left| \\langle 0 | \\mathbf{U}^\\dagger_{\\Phi(\\mathbf{x_j})} \\mathbf{U}_{\\Phi(\\mathbf{x_i})} | 0 \\rangle \\right|^2 $.\n",
    "    2. For each training datapoint $\\mathbf{x_i}$ and testing point $\\mathbf{y_i}$, apply the feature map and measure the transition probability: $ K_{ij} = \\left| \\langle 0 | \\mathbf{U}^\\dagger_{\\Phi(\\mathbf{y_i})} \\mathbf{U}_{\\Phi(\\mathbf{x_i})} | 0 \\rangle \\right|^2 $.\n",
    "2. Use the train and test quantum kernel matrices in a classical support vector machine classification algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c834dc-58ba-400c-8f17-31eca523ae3c",
   "metadata": {},
   "source": [
    "The `scikit-learn` `svc` algorithm allows us to define a [custom kernel](https://scikit-learn.org/stable/modules/svm.html#custom-kernels) in two ways: by providing the kernel as a callable function or by precomputing the kernel matrix. We can do either of these using the `QuantumKernel` class in Qiskit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70da4160-248c-4e5b-a4a6-80f57a146d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## WRITE YOUR CODE HERE (SVC)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7996460-252a-4a27-aa96-b409b424a5d3",
   "metadata": {},
   "source": [
    "When done, use the cell below to see how your model compares to classical SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d617b033-581e-440b-8fca-609329c8a2c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "classical_kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "\n",
    "for kernel in classical_kernels:\n",
    "    classical_svc = SVC(kernel=kernel, decision_function_shape=\"ovr\")\n",
    "    classical_svc.fit([i.flatten() for i in X_train], Y_train)\n",
    "    classical_score = classical_svc.score([i.flatten() for i in X_test], Y_test)\n",
    "\n",
    "    print('%s kernel classification test score:  %0.2f' % (kernel, classical_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bab788-9dc0-40ff-b18c-dbd66ea44b90",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
